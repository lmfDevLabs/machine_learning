# -*- coding: utf-8 -*-
"""Drugs-kMeas.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RmxKurtJ9nSpUGFZEhC_tNDyvUNaFbA8

1. Cargar y explorar los datos
"""

from google.colab import drive
drive.mount('/content/drive')
import pandas as pd

# Cambia la ruta según donde tengas almacenado tu archivo
file_path = '/content/drive/MyDrive/Colab Notebooks/ML/drug_consumption.data'
data = pd.read_csv(file_path, header=None)

# Mostrar las primeras filas para confirmar que se ha cargado correctamente
print(data.head())

"""2. Preprocesamiento de los datos: Nombrar columnas y convertir categorías"""

# Definir los nombres de las columnas
column_names = [
    "ID", "Age", "Gender", "Education", "Country", "Ethnicity",
    "Nscore", "Escore", "Oscore", "Ascore", "Cscore", "Impulsiveness", "SS",
    "Alcohol", "Amphetamines", "AmylNitrite", "Benzodiazepine", "Cannabis",
    "Chocolate", "Cocaine", "Caffeine", "Crack", "Ecstasy", "Heroin",
    "Ketamine", "LegalHighs", "LSD", "Methadone", "Mushrooms",
    "Nicotine", "VolatileSubstance", "Semeron"
]

# Asignar los nombres de las columnas al dataframe
data.columns = column_names

# Convertir las categorías de consumo de drogas a valores binarios
for drug in column_names[13:]:
    data[drug] = (data[drug] != 'CL0').astype(int)

# Mostrar las primeras filas después de la conversión
print(data.head())

"""3. Normalización de los datos"""

from sklearn.preprocessing import StandardScaler

# Seleccionar las columnas que describen a los individuos (las primeras 12 columnas)
features = data.columns[1:13]

# Normalizar estas características
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data[features])

# Crear un nuevo dataframe con los datos normalizados y las columnas de consumo de drogas
data_normalized = pd.DataFrame(data_scaled, columns=features)
data_normalized = pd.concat([data_normalized, data.iloc[:, 13:]], axis=1)

# Mostrar las primeras filas del dataframe normalizado
print(data_normalized.head())

"""4. Implementación del método del codo para K-means (opcional para prueba reducida)"""

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Determinar el número óptimo de clusters usando el método del codo
inertias = []
k_values = range(1, 11)  # Puedes ajustar este rango según tu necesidad

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(data_normalized)
    inertias.append(kmeans.inertia_)

# Graficar el método del codo
plt.figure(figsize=(10, 6))
plt.plot(k_values, inertias, marker='o')
plt.title('Método del Codo para K-means Clustering')
plt.xlabel('Número de Clusters')
plt.ylabel('Inercia')
plt.xticks(k_values)
plt.grid(True)
plt.show()

"""5. Silueta"""

from sklearn.metrics import silhouette_samples, silhouette_score
import matplotlib.cm as cm
import numpy as np

def plot_silhouette(data, n_clusters):
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    cluster_labels = kmeans.fit_predict(data)
    silhouette_avg = silhouette_score(data, cluster_labels)
    print("Para n_clusters =", n_clusters, "El promedio de la silueta es :", silhouette_avg)

    # Calcular los valores de silueta para cada muestra
    sample_silhouette_values = silhouette_samples(data, cluster_labels)

    y_lower = 10
    fig, ax = plt.subplots()

    for i in range(n_clusters):
        # Agregar los valores de silueta del i-ésimo cluster y ordenarlos
        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]
        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = cm.nipy_spectral(float(i) / n_clusters)
        ax.fill_betweenx(np.arange(y_lower, y_upper),
                         0, ith_cluster_silhouette_values,
                         facecolor=color, edgecolor=color, alpha=0.7)

        # Etiquetar los gráficos de silueta con el número del cluster en el centro
        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        # Calcular el nuevo y_lower para el próximo gráfico
        y_lower = y_upper + 10

    ax.set_title("Gráfico de la Silueta para los diferentes clusters.")
    ax.set_xlabel("Valor de la Silueta")
    ax.set_ylabel("Número del Cluster")

    # Línea vertical para el promedio de la silueta de todos los valores
    ax.axvline(x=silhouette_avg, color="red", linestyle="--")

    ax.set_yticks([])  # Limpiar las etiquetas de y
    ax.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

    plt.show()

# Ejemplo de uso para K=3
plot_silhouette(data_normalized, 3)

from sklearn.metrics import silhouette_score

# Asumiendo que ya hemos importado KMeans y que tenemos 'data_normalized'
silhouette_scores = []
k_values = range(2, 11)  # Empezando desde 2 porque la silueta no se puede calcular con un solo cluster

for k in k_values:
    kmeans = KMeans(n_clusters=k, random_state=42)
    cluster_labels = kmeans.fit_predict(data_normalized)
    silhouette_avg = silhouette_score(data_normalized, cluster_labels)
    silhouette_scores.append(silhouette_avg)

# Graficar los valores de silueta para cada K
plt.figure(figsize=(10, 6))
plt.plot(k_values, silhouette_scores, marker='o')
plt.title('Análisis de Silueta para Diferentes Valores de K')
plt.xlabel('Número de Clusters')
plt.ylabel('Puntaje de Silueta')
plt.xticks(k_values)
plt.grid(True)
plt.show()

"""PCA"""

import numpy as np
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# Normalizar los datos antes de calcular la matriz de covarianza
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data_normalized)  # Asumiendo que data_normalized son tus datos

# Calcular la matriz de covarianza
cov_matrix = np.cov(data_scaled, rowvar=False)
print("Matriz de Covarianza:\n", cov_matrix)

# Realizar PCA
pca = PCA()
pca.fit(data_scaled)

# Calcular la varianza explicada acumulativa
varianza_explicada_acumulativa = np.cumsum(pca.explained_variance_ratio_)
print("Varianza Explicada Acumulativa:\n", varianza_explicada_acumulativa)

# Encontrar el número mínimo de componentes que explican al menos el 80% de la varianza
num_components_80 = np.where(varianza_explicada_acumulativa >= 0.8)[0][0] + 1  # +1 porque los índices en Python empiezan en 0
print("Número de componentes necesarios para explicar al menos el 80% de la varianza:", num_components_80)

# Opcionalmente, aplicar PCA con el número de componentes encontrado
pca_reducido = PCA(n_components=num_components_80)
data_pca_reducido = pca_reducido.fit_transform(data_scaled)

from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D  # Necesario para gráficos 3D

# Aplicar PCA para reducir la dimensionalidad a n componentes
pca = PCA(n_components=17)
data_pca_3d = pca.fit_transform(data_normalized)

# Asumamos que decidimos usar K-means con, por ejemplo, 3 clusters
kmeans = KMeans(n_clusters=3, random_state=42)
cluster_labels = kmeans.fit_predict(data_normalized)

# Configurar un gráfico 3D
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')

# Graficar los resultados
scatter = ax.scatter(data_pca_3d[:, 0], data_pca_3d[:, 1], data_pca_3d[:, 2],
                     c=cluster_labels, cmap='viridis', edgecolor='k', s=50)

# Etiquetas y título
ax.set_title('Visualización de Clusters con 3 Componentes Principales')
ax.set_xlabel('Componente Principal 1')
ax.set_ylabel('Componente Principal 2')
ax.set_zlabel('Componente Principal 3')

# Añadir una barra de colores para mostrar la pertenencia a los clusters
plt.colorbar(scatter)

plt.show()